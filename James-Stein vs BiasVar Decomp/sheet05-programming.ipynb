{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: The James-Stein Estimator (20 P)\n",
    "\n",
    "Let $x_1,\\dots,x_N \\in \\mathbb{R}^d$ be independent draws from a multivariate Gaussian distribution with mean vector $\\mu$ and covariance matrix $\\Sigma = \\sigma^2 I$. It can be shown that the maximum-likelihood estimator of the mean parameter $\\mu$ is the empirical mean given by:\n",
    "$$\n",
    "\\hat \\mu_\\text{ML} = \\frac1N \\sum_{i=1}^N x_i\n",
    "$$\n",
    "Maximum-likelihood appears to be a strong estimator. However, it was demonstrated that the following estimator\n",
    "$$\n",
    "\\hat \\mu_{JS} = \\Big(1-\\frac{(d-2) \\cdot \\frac{\\sigma^2}{N}}{\\|\\hat{\\mu}_\\text{ML}\\|^2}\\Big) \\hat{\\mu}_\\text{ML}\n",
    "$$\n",
    "(a shrinked version of the maximum-likelihood estimator towards the origin) has actually a smaller distance from the true mean when $d \\geq 3$. This however assumes knowledge of the variance of the distribution for which the mean is estimated. This estimator is called the James-Stein estimator. While the proof is a bit involved, this fact can be easily demonstrated empirically through simulation. This is the object of this exercise.\n",
    "\n",
    "The code below draws ten 50-dimensional points from a normal distribution with mean vector $\\mu = (1,\\dots,1)$ and covariance $\\Sigma = I$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "def getdata(seed):\n",
    "\n",
    "    n = 10              # data points\n",
    "    d = 50              # dimensionality of data\n",
    "    m = numpy.ones([d]) # true mean\n",
    "    s = 1.0             # true standard deviation\n",
    "\n",
    "    rstate = numpy.random.mtrand.RandomState(seed)\n",
    "    X = rstate.normal(0,1,[n,d])*s+m\n",
    "    \n",
    "    return X,m,s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function computes the maximum likelihood estimator from a sample of the data assumed to be generated by a Gaussian distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ML(X):\n",
    "    return X.mean(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing the James-Stein Estimator (10 P)\n",
    "\n",
    "* **Based on the ML estimator function, write a function that receives as input the data $(X_i)_{i=1}^n$ and the (known) variance $\\sigma^2$ of the generating distribution, and computes the James-Stein estimator**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def JS(X,s):\n",
    "    # REPLACE BY YOUR CODE\n",
    "    import solutions\n",
    "    m_JS = solutions.JS(X,s)\n",
    "    ###\n",
    "    return m_JS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing the ML and James-Stein Estimators (10 P)\n",
    "\n",
    "We would like to compute the error of the maximum likelihood estimator and the James-Stein estimator for 100 different samples (where each sample consists of 10 draws generated by the function `getdata` with a different random seed). Here, for reproducibility, we use seeds from 0 to 99. The error should be measured as the Euclidean distance between the true mean vector and the estimated mean vector.\n",
    "\n",
    "* **Compute the maximum-likelihood and James-Stein estimations.**\n",
    "* **Measure the error of these estimations.**\n",
    "* **Build a scatter plot comparing these errors for different samples.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "### REPLACE BY YOUR CODE\n",
    "import solutions\n",
    "solutions.compare_ML_JS()\n",
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Bias/Variance Decomposition (30 P)\n",
    "\n",
    "In this part, we would like to implement a procedure to find the bias and variance of different predictors. We consider one for regression and one for classification. These predictors are available in the module utils.\n",
    "\n",
    "* **`utils.ParzenRegressor`:** A regression method based on Parzen window. The hyperparameter corresponds to the scale of the Parzen window. A large scale creates a more rigid model. A small scale creates a more flexible one.\n",
    "\n",
    "* **`utils.ParzenClassifier`:** A classification method based on Parzen window. The hyperparameter corresponds to the scale of the Parzen window. A large scale creates a more rigid model. A small scale creates a more flexible one. Note that instead of returning a single class for a given data point, it outputs a probability distribution over the set of possible classes.\n",
    "\n",
    "Each class of predictor implements the following three methods:\n",
    "\n",
    "  - **`__init__(self,parameter):`** Create an instance of the predictor with a certain scale parameter.\n",
    "\n",
    "  - **`fit(self,X,T):`** Fit the predictor to the data (a set of data points `X` and targets `T`).\n",
    "    \n",
    "  - **`predict(self,X):`** Compute the output values arbitrary inputs `X`.\n",
    "  \n",
    "  \n",
    "To compute the bias and variance estimates, we require *multiple samples* from the training set for a single set of observation data. To acomplish this, we utilize the **`Sampler`** class provided. The sampler is initialized with the training data and passed to the method for estimating bias and variance, where its function **`sampler.sample()`** is called repeatedly in order to fit multiple models and create an ensemble of prediction for each test data point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression Case (15 P)\n",
    "\n",
    "For the regression case, Bias, Variance and Error are given by:\n",
    "\n",
    " * $\\mathrm{Bias}(Y)^2 = (\\mathbb{E}_Y [ Y - T ])^2$\n",
    " * $\\mathrm{Var}(Y)$ $= \\mathbb{E}_Y [(Y - \\mathbb{E}_Y[Y])^2 ]$\n",
    " * $\\mathrm{Error}(Y)$ = $\\mathbb{E}_Y[(Y-T)^2]$\n",
    " \n",
    "**Task:** Implement the KL-based Bias-Variance Decomposition defined above. The function should repeatedly sample training sets from the sampler (as many times as specified by the argument nbsamples), learn the predictor on them, and evaluate the variance on the out-of-sample distribution given by X and T."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def biasVarianceRegression(sampler, predictor, X, T, nbsamples):\n",
    "    \n",
    "    # --------------------------------\n",
    "    # TODO: REPLACE BY YOUR CODE\n",
    "    # --------------------------------\n",
    "    import solutions\n",
    "    bias,variance = solutions.biasVarianceRegression(sampler, predictor, X, T, nbsamples=nbsamples)\n",
    "    # --------------------------------\n",
    "    \n",
    "    return bias,variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your implementation can be tested with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils,numpy\n",
    "%matplotlib inline\n",
    "utils.plotBVE(utils.Housing,numpy.logspace(-6,3,num=30),utils.ParzenRegressor,biasVarianceRegression,'Housing Regression')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Case (15 P)\n",
    "\n",
    "We consider here the Kullback-Leibler divergence as a measure of classification error, as derived in the exercise, the Bias, Variance decomposition for such error is:\n",
    "\n",
    "- $\\mathrm{Bias}(Y) = D_\\mathrm{KL}(T||R)$\n",
    "\n",
    "- $\\mathrm{Var}(Y) = \\mathbb{E}_Y[D_\\mathrm{KL}(R||Y)]$\n",
    "\n",
    "- $\\mathrm{Error}(Y) = \\mathbb{E}_Y[D_\\mathrm{KL}(T||Y)]$\n",
    "\n",
    "where $R$ is the distribution that minimizes its expected KL divergence from the estimator of probability distribution $Y$ (see the theoretical exercise for how it is computed exactly), and where $T$ is the target class distribution.\n",
    "\n",
    "**Task:** Implement the KL-based Bias-Variance Decomposition defined above. The function should repeatedly sample training sets from the sampler (as many times as specified by the argument nbsamples), learn the predictor on them, and evaluate the variance on the out-of-sample distribution given by X and T."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def biasVarianceClassification(sampler, predictor, X, T, nbsamples=25):\n",
    "    \n",
    "    # --------------------------------\n",
    "    # TODO: REPLACE BY YOUR CODE\n",
    "    # --------------------------------\n",
    "    import solutions\n",
    "    bias,variance = solutions.biasVarianceClassification(sampler, predictor, X, T, nbsamples=nbsamples)\n",
    "    # --------------------------------\n",
    "    \n",
    "    return bias,variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your implementation can be tested with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils,numpy\n",
    "%matplotlib inline\n",
    "utils.plotBVE(utils.Yeast,numpy.logspace(-6,3,num=30),utils.ParzenClassifier,biasVarianceClassification,'Yeast Classification')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
